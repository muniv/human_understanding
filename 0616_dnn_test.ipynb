{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataload and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, roc_auc_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_hr_user01_06_path = r'C:\\Users\\user\\Desktop\\work\\AI_study\\korea_univ_std\\ETRI_human\\test_code\\my_test_model\\acc_hr_user01_06'\n",
    "\n",
    "acc_hr_user01_06_df = pd.DataFrame()\n",
    "\n",
    "# 디렉토리의 모든 파일을 순회하며 DataFrame에 추가\n",
    "for file in os.listdir(acc_hr_user01_06_path):\n",
    "    file_path = os.path.join(acc_hr_user01_06_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    acc_hr_user01_06_df = pd.concat([acc_hr_user01_06_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "acc_hr_user01_06_df = acc_hr_user01_06_df.dropna()\n",
    "\n",
    "# drop column\n",
    "acc_hr_user01_06_df.drop(columns=['Unnamed: 0.1','Unnamed: 0','subject_id'],inplace=True)\n",
    "\n",
    "# datetime set\n",
    "acc_hr_user01_06_df['date'] = pd.to_datetime(acc_hr_user01_06_df['date'])\n",
    "acc_hr_user01_06_df['date'] = (acc_hr_user01_06_df['date'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_hr_user01_06_df_sds = pd.DataFrame()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "acc_hr_user01_06_df_sds[list(acc_hr_user01_06_df.columns[:30])] = scaler.fit_transform(acc_hr_user01_06_df.iloc[:,:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = acc_hr_user01_06_df.iloc[:,31:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(acc_hr_user01_06_df_sds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(64, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.1)\n",
    "        \n",
    "        self.output = nn.Linear(64, y_train.shape[1])\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.dropout4(self.relu4(self.layer4(x)))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6881439685821533\n",
      "Epoch 2, Loss: 0.6855859756469727\n",
      "Epoch 3, Loss: 0.6925676465034485\n",
      "Epoch 4, Loss: 0.7041584849357605\n",
      "Epoch 5, Loss: 0.6552127003669739\n",
      "Epoch 6, Loss: 0.6087868213653564\n",
      "Epoch 7, Loss: 0.5735003352165222\n",
      "Epoch 8, Loss: 0.630150556564331\n",
      "Epoch 9, Loss: 0.6339369416236877\n",
      "Epoch 10, Loss: 0.6107613444328308\n",
      "Epoch 11, Loss: 0.5932239890098572\n",
      "Epoch 12, Loss: 0.6608797907829285\n",
      "Epoch 13, Loss: 0.5805125832557678\n",
      "Epoch 14, Loss: 0.6061140298843384\n",
      "Epoch 15, Loss: 0.45783472061157227\n",
      "Epoch 16, Loss: 0.5007861256599426\n",
      "Epoch 17, Loss: 0.4715280830860138\n",
      "Epoch 18, Loss: 0.6006022095680237\n",
      "Epoch 19, Loss: 0.5645208358764648\n",
      "Epoch 20, Loss: 0.48507484793663025\n",
      "Epoch 21, Loss: 0.4564422369003296\n",
      "Epoch 22, Loss: 0.4738701283931732\n",
      "Epoch 23, Loss: 0.4898134469985962\n",
      "Epoch 24, Loss: 0.4111727476119995\n",
      "Epoch 25, Loss: 0.474274218082428\n",
      "Epoch 26, Loss: 0.3912581205368042\n",
      "Epoch 27, Loss: 0.34392619132995605\n",
      "Epoch 28, Loss: 0.3849621117115021\n",
      "Epoch 29, Loss: 0.26702502369880676\n",
      "Epoch 30, Loss: 0.290188729763031\n",
      "Epoch 31, Loss: 0.303338885307312\n",
      "Epoch 32, Loss: 0.32903990149497986\n",
      "Epoch 33, Loss: 0.2605612576007843\n",
      "Epoch 34, Loss: 0.22604884207248688\n",
      "Epoch 35, Loss: 0.258865088224411\n",
      "Epoch 36, Loss: 0.20987117290496826\n",
      "Epoch 37, Loss: 0.1520504653453827\n",
      "Epoch 38, Loss: 0.16423764824867249\n",
      "Epoch 39, Loss: 0.2703927755355835\n",
      "Epoch 40, Loss: 0.2607181966304779\n",
      "Epoch 41, Loss: 0.28230878710746765\n",
      "Epoch 42, Loss: 0.22059158980846405\n",
      "Epoch 43, Loss: 0.15165473520755768\n",
      "Epoch 44, Loss: 0.2006704956293106\n",
      "Epoch 45, Loss: 0.16972611844539642\n",
      "Epoch 46, Loss: 0.13519933819770813\n",
      "Epoch 47, Loss: 0.1110326424241066\n",
      "Epoch 48, Loss: 0.10168412327766418\n",
      "Epoch 49, Loss: 0.0714138075709343\n",
      "Epoch 50, Loss: 0.11308473348617554\n",
      "Epoch 51, Loss: 0.14708012342453003\n",
      "Epoch 52, Loss: 0.1433591991662979\n",
      "Epoch 53, Loss: 0.09247143566608429\n",
      "Epoch 54, Loss: 0.16889935731887817\n",
      "Epoch 55, Loss: 0.07731856405735016\n",
      "Epoch 56, Loss: 0.16027505695819855\n",
      "Epoch 57, Loss: 0.07390623539686203\n",
      "Epoch 58, Loss: 0.0466705821454525\n",
      "Epoch 59, Loss: 0.04403439164161682\n",
      "Epoch 60, Loss: 0.09111098945140839\n",
      "Epoch 61, Loss: 0.05844680592417717\n",
      "Epoch 62, Loss: 0.11081783473491669\n",
      "Epoch 63, Loss: 0.04028098285198212\n",
      "Epoch 64, Loss: 0.021657777950167656\n",
      "Epoch 65, Loss: 0.06969267129898071\n",
      "Epoch 66, Loss: 0.07420540601015091\n",
      "Epoch 67, Loss: 0.04804539680480957\n",
      "Epoch 68, Loss: 0.08613655716180801\n",
      "Epoch 69, Loss: 0.08849101513624191\n",
      "Epoch 70, Loss: 0.08153796941041946\n",
      "Epoch 71, Loss: 0.03032568469643593\n",
      "Epoch 72, Loss: 0.04882591590285301\n",
      "Epoch 73, Loss: 0.025685161352157593\n",
      "Epoch 74, Loss: 0.038872603327035904\n",
      "Epoch 75, Loss: 0.09411652386188507\n",
      "Epoch 76, Loss: 0.023274315521121025\n",
      "Epoch 77, Loss: 0.02907371334731579\n",
      "Epoch 78, Loss: 0.060190968215465546\n",
      "Epoch 79, Loss: 0.07542550563812256\n",
      "Epoch 80, Loss: 0.11958621442317963\n",
      "Epoch 81, Loss: 0.11444613337516785\n",
      "Epoch 82, Loss: 0.04165009409189224\n",
      "Epoch 83, Loss: 0.04304684326052666\n",
      "Epoch 84, Loss: 0.06212645024061203\n",
      "Epoch 85, Loss: 0.06654819846153259\n",
      "Epoch 86, Loss: 0.07514125853776932\n",
      "Epoch 87, Loss: 0.029312577098608017\n",
      "Epoch 88, Loss: 0.025676194578409195\n",
      "Epoch 89, Loss: 0.04203706979751587\n",
      "Epoch 90, Loss: 0.023171991109848022\n",
      "Epoch 91, Loss: 0.026751289144158363\n",
      "Epoch 92, Loss: 0.04036435857415199\n",
      "Epoch 93, Loss: 0.1302584707736969\n",
      "Epoch 94, Loss: 0.03415996581315994\n",
      "Epoch 95, Loss: 0.023061370477080345\n",
      "Epoch 96, Loss: 0.08998780697584152\n",
      "Epoch 97, Loss: 0.016603251919150352\n",
      "Epoch 98, Loss: 0.015142219141125679\n",
      "Epoch 99, Loss: 0.04488706961274147\n",
      "Epoch 100, Loss: 0.033861711621284485\n",
      "Epoch 101, Loss: 0.009743470698595047\n",
      "Epoch 102, Loss: 0.018288785591721535\n",
      "Epoch 103, Loss: 0.012496468611061573\n",
      "Epoch 104, Loss: 0.016276726499199867\n",
      "Epoch 105, Loss: 0.010460441000759602\n",
      "Epoch 106, Loss: 0.021425707265734673\n",
      "Epoch 107, Loss: 0.03439750149846077\n",
      "Epoch 108, Loss: 0.01328155118972063\n",
      "Epoch 109, Loss: 0.062451381236314774\n",
      "Epoch 110, Loss: 0.02502613328397274\n",
      "Epoch 111, Loss: 0.007609161082655191\n",
      "Epoch 112, Loss: 0.09111187607049942\n",
      "Epoch 113, Loss: 0.07897486537694931\n",
      "Epoch 114, Loss: 0.01606503501534462\n",
      "Epoch 115, Loss: 0.012066386640071869\n",
      "Epoch 116, Loss: 0.016680167987942696\n",
      "Epoch 117, Loss: 0.013937951065599918\n",
      "Epoch 118, Loss: 0.00266073364764452\n",
      "Epoch 119, Loss: 0.015951473265886307\n",
      "Epoch 120, Loss: 0.016114160418510437\n",
      "Epoch 121, Loss: 0.0029925236012786627\n",
      "Epoch 122, Loss: 0.03754013031721115\n",
      "Epoch 123, Loss: 0.038794562220573425\n",
      "Epoch 124, Loss: 0.020066848024725914\n",
      "Epoch 125, Loss: 0.010113001801073551\n",
      "Epoch 126, Loss: 0.008466344326734543\n",
      "Epoch 127, Loss: 0.01206484716385603\n",
      "Epoch 128, Loss: 0.005857498850673437\n",
      "Epoch 129, Loss: 0.005297190044075251\n",
      "Epoch 130, Loss: 0.013267438858747482\n",
      "Epoch 131, Loss: 0.005637824535369873\n",
      "Epoch 132, Loss: 0.01969645544886589\n",
      "Epoch 133, Loss: 0.0035035712644457817\n",
      "Epoch 134, Loss: 0.028136160224676132\n",
      "Epoch 135, Loss: 0.011204474605619907\n",
      "Epoch 136, Loss: 0.02712319605052471\n",
      "Epoch 137, Loss: 0.018578339368104935\n",
      "Epoch 138, Loss: 0.009939494542777538\n",
      "Epoch 139, Loss: 0.026967518031597137\n",
      "Epoch 140, Loss: 0.008515716530382633\n",
      "Epoch 141, Loss: 0.008437291719019413\n",
      "Epoch 142, Loss: 0.009585641324520111\n",
      "Epoch 143, Loss: 0.004387548193335533\n",
      "Epoch 144, Loss: 0.0016342538874596357\n",
      "Epoch 145, Loss: 0.013132787309587002\n",
      "Epoch 146, Loss: 0.0035207155160605907\n",
      "Epoch 147, Loss: 0.007571520283818245\n",
      "Epoch 148, Loss: 0.0025383508764207363\n",
      "Epoch 149, Loss: 0.005182737018913031\n",
      "Epoch 150, Loss: 0.01779255084693432\n",
      "Epoch 151, Loss: 0.008835135027766228\n",
      "Epoch 152, Loss: 0.0024002811405807734\n",
      "Epoch 153, Loss: 0.004525148309767246\n",
      "Epoch 154, Loss: 0.019666841253638268\n",
      "Epoch 155, Loss: 0.0048779090866446495\n",
      "Epoch 156, Loss: 0.002089815679937601\n",
      "Epoch 157, Loss: 0.00189078482799232\n",
      "Epoch 158, Loss: 0.003126207971945405\n",
      "Epoch 159, Loss: 0.028940366581082344\n",
      "Epoch 160, Loss: 0.08559709787368774\n",
      "Epoch 161, Loss: 0.027012070640921593\n",
      "Epoch 162, Loss: 0.02610832080245018\n",
      "Epoch 163, Loss: 0.005715352948755026\n",
      "Epoch 164, Loss: 0.014026879332959652\n",
      "Epoch 165, Loss: 0.012483855709433556\n",
      "Epoch 166, Loss: 0.002928155707195401\n",
      "Epoch 167, Loss: 0.0178920216858387\n",
      "Epoch 168, Loss: 0.014887218363583088\n",
      "Epoch 169, Loss: 0.007038651499897242\n",
      "Epoch 170, Loss: 0.001321413554251194\n",
      "Epoch 171, Loss: 0.014701194129884243\n",
      "Epoch 172, Loss: 0.009489213116466999\n",
      "Epoch 173, Loss: 0.0036870138719677925\n",
      "Epoch 174, Loss: 0.00023954744392540306\n",
      "Epoch 175, Loss: 0.003372038481757045\n",
      "Epoch 176, Loss: 0.002165849320590496\n",
      "Epoch 177, Loss: 0.006446250714361668\n",
      "Epoch 178, Loss: 0.007456147111952305\n",
      "Epoch 179, Loss: 0.00836880225688219\n",
      "Epoch 180, Loss: 0.0015548078808933496\n",
      "Epoch 181, Loss: 0.020850341767072678\n",
      "Epoch 182, Loss: 0.003802334889769554\n",
      "Epoch 183, Loss: 0.011651571840047836\n",
      "Epoch 184, Loss: 0.003249300178140402\n",
      "Epoch 185, Loss: 0.006714873947203159\n",
      "Epoch 186, Loss: 0.003658507950603962\n",
      "Epoch 187, Loss: 0.0010575552005320787\n",
      "Epoch 188, Loss: 0.034759748727083206\n",
      "Epoch 189, Loss: 0.023007724434137344\n",
      "Epoch 190, Loss: 0.007810652256011963\n",
      "Epoch 191, Loss: 0.023464761674404144\n",
      "Epoch 192, Loss: 0.009620895609259605\n",
      "Epoch 193, Loss: 0.004512140527367592\n",
      "Epoch 194, Loss: 0.019340287894010544\n",
      "Epoch 195, Loss: 0.006229923572391272\n",
      "Epoch 196, Loss: 0.017552118748426437\n",
      "Epoch 197, Loss: 0.008121892809867859\n",
      "Epoch 198, Loss: 0.00654742494225502\n",
      "Epoch 199, Loss: 0.0005366628174670041\n",
      "Epoch 200, Loss: 0.007293371483683586\n",
      "Epoch 201, Loss: 0.004539798945188522\n",
      "Epoch 202, Loss: 0.010331174358725548\n",
      "Epoch 203, Loss: 0.006659212987869978\n",
      "Epoch 204, Loss: 0.0011244991328567266\n",
      "Epoch 205, Loss: 0.0020390970166772604\n",
      "Epoch 206, Loss: 0.003658212488517165\n",
      "Epoch 207, Loss: 0.006722454912960529\n",
      "Epoch 208, Loss: 0.0038227608893066645\n",
      "Epoch 209, Loss: 0.006868529133498669\n",
      "Epoch 210, Loss: 0.00456369761377573\n",
      "Epoch 211, Loss: 0.011630172841250896\n",
      "Epoch 212, Loss: 0.0031350443605333567\n",
      "Epoch 213, Loss: 0.010717309080064297\n",
      "Epoch 214, Loss: 0.0032976865768432617\n",
      "Epoch 215, Loss: 0.0019653949420899153\n",
      "Epoch 216, Loss: 0.0014763870276510715\n",
      "Epoch 217, Loss: 0.010103625245392323\n",
      "Epoch 218, Loss: 0.010906144976615906\n",
      "Epoch 219, Loss: 0.006232369691133499\n",
      "Epoch 220, Loss: 0.009988775476813316\n",
      "Epoch 221, Loss: 0.0013395344140008092\n",
      "Epoch 222, Loss: 0.0028621077071875334\n",
      "Epoch 223, Loss: 0.01675359345972538\n",
      "Epoch 224, Loss: 0.001029353472404182\n",
      "Epoch 225, Loss: 0.003855921095237136\n",
      "Epoch 226, Loss: 0.007817763835191727\n",
      "Epoch 227, Loss: 0.018197886645793915\n",
      "Epoch 228, Loss: 0.044954776763916016\n",
      "Epoch 229, Loss: 0.015099124051630497\n",
      "Epoch 230, Loss: 0.0006820498965680599\n",
      "Epoch 231, Loss: 0.004925293382257223\n",
      "Epoch 232, Loss: 0.005115407984703779\n",
      "Epoch 233, Loss: 0.007305717561393976\n",
      "Epoch 234, Loss: 0.03211796283721924\n",
      "Epoch 235, Loss: 0.0032664481550455093\n",
      "Epoch 236, Loss: 0.0008574356324970722\n",
      "Epoch 237, Loss: 0.020181141793727875\n",
      "Epoch 238, Loss: 0.003293936373665929\n",
      "Epoch 239, Loss: 0.0056036729365587234\n",
      "Epoch 240, Loss: 0.006785841658711433\n",
      "Epoch 241, Loss: 0.014893894083797932\n",
      "Epoch 242, Loss: 0.006600281223654747\n",
      "Epoch 243, Loss: 0.0034060580655932426\n",
      "Epoch 244, Loss: 0.0011452487669885159\n",
      "Epoch 245, Loss: 0.0005395863554440439\n",
      "Epoch 246, Loss: 0.007448509801179171\n",
      "Epoch 247, Loss: 0.0014416611520573497\n",
      "Epoch 248, Loss: 0.002615999896079302\n",
      "Epoch 249, Loss: 0.00802638754248619\n",
      "Epoch 250, Loss: 0.036760568618774414\n",
      "Epoch 251, Loss: 0.008761206641793251\n",
      "Epoch 252, Loss: 0.004603963810950518\n",
      "Epoch 253, Loss: 0.013260142877697945\n",
      "Epoch 254, Loss: 0.01894310861825943\n",
      "Epoch 255, Loss: 0.011713314801454544\n",
      "Epoch 256, Loss: 0.01153390109539032\n",
      "Epoch 257, Loss: 0.014100603759288788\n",
      "Epoch 258, Loss: 0.008572421967983246\n",
      "Epoch 259, Loss: 0.00585928512737155\n",
      "Epoch 260, Loss: 0.002023867331445217\n",
      "Epoch 261, Loss: 0.02436949498951435\n",
      "Epoch 262, Loss: 0.0010766788618639112\n",
      "Epoch 263, Loss: 0.0008464418933726847\n",
      "Epoch 264, Loss: 0.021567469462752342\n",
      "Epoch 265, Loss: 0.0009679859504103661\n",
      "Epoch 266, Loss: 0.007665068376809359\n",
      "Epoch 267, Loss: 0.0038554309867322445\n",
      "Epoch 268, Loss: 0.04083838686347008\n",
      "Epoch 269, Loss: 0.001033650478348136\n",
      "Epoch 270, Loss: 0.012485701590776443\n",
      "Epoch 271, Loss: 0.0027397917583584785\n",
      "Epoch 272, Loss: 0.0005079242400825024\n",
      "Epoch 273, Loss: 0.007899655029177666\n",
      "Epoch 274, Loss: 0.005850510206073523\n",
      "Epoch 275, Loss: 0.04744705930352211\n",
      "Epoch 276, Loss: 0.12826985120773315\n",
      "Epoch 277, Loss: 0.010969001799821854\n",
      "Epoch 278, Loss: 0.09149147570133209\n",
      "Epoch 279, Loss: 0.05125861614942551\n",
      "Epoch 280, Loss: 0.002233160426840186\n",
      "Epoch 281, Loss: 0.004266209900379181\n",
      "Epoch 282, Loss: 0.004964128602296114\n",
      "Epoch 283, Loss: 0.006819794420152903\n",
      "Epoch 284, Loss: 0.0063612377271056175\n",
      "Epoch 285, Loss: 0.0010376906720921397\n",
      "Epoch 286, Loss: 0.00116723810788244\n",
      "Epoch 287, Loss: 0.049934256821870804\n",
      "Epoch 288, Loss: 0.0018301968229934573\n",
      "Epoch 289, Loss: 0.0008684875792823732\n",
      "Epoch 290, Loss: 0.0013098102062940598\n",
      "Epoch 291, Loss: 0.006206229794770479\n",
      "Epoch 292, Loss: 0.0035399100743234158\n",
      "Epoch 293, Loss: 0.0066305710934102535\n",
      "Epoch 294, Loss: 0.003555475967004895\n",
      "Epoch 295, Loss: 0.000677471689414233\n",
      "Epoch 296, Loss: 0.00478067621588707\n",
      "Epoch 297, Loss: 0.005148768424987793\n",
      "Epoch 298, Loss: 0.007257463876157999\n",
      "Epoch 299, Loss: 0.0022330228239297867\n",
      "Epoch 300, Loss: 0.002502509392797947\n",
      "Epoch 301, Loss: 0.005333098117262125\n",
      "Epoch 302, Loss: 0.016772570088505745\n",
      "Epoch 303, Loss: 0.010807783342897892\n",
      "Epoch 304, Loss: 0.007901182398200035\n",
      "Epoch 305, Loss: 0.0034443007316440344\n",
      "Epoch 306, Loss: 0.004217438865453005\n",
      "Epoch 307, Loss: 0.00042356064659543335\n",
      "Epoch 308, Loss: 0.0007388865342363715\n",
      "Epoch 309, Loss: 0.0014363533118739724\n",
      "Epoch 310, Loss: 0.0023538554087281227\n",
      "Epoch 311, Loss: 0.0032093068584799767\n",
      "Epoch 312, Loss: 0.0005474389181472361\n",
      "Epoch 313, Loss: 0.003024844452738762\n",
      "Epoch 314, Loss: 0.0007689471822232008\n",
      "Epoch 315, Loss: 0.00013728432531934232\n",
      "Epoch 316, Loss: 0.001406530849635601\n",
      "Epoch 317, Loss: 0.0024531143717467785\n",
      "Epoch 318, Loss: 0.0073388791643083096\n",
      "Epoch 319, Loss: 0.002969910390675068\n",
      "Epoch 320, Loss: 0.002996925264596939\n",
      "Epoch 321, Loss: 0.00033044395968317986\n",
      "Epoch 322, Loss: 0.0008466586587019265\n",
      "Epoch 323, Loss: 0.0006001383881084621\n",
      "Epoch 324, Loss: 0.0031860671006143093\n",
      "Epoch 325, Loss: 0.004384193103760481\n",
      "Epoch 326, Loss: 0.0006622821092605591\n",
      "Epoch 327, Loss: 0.0012501170858740807\n",
      "Epoch 328, Loss: 0.0005714802537113428\n",
      "Epoch 329, Loss: 0.0033194837160408497\n",
      "Epoch 330, Loss: 0.0022085197269916534\n",
      "Epoch 331, Loss: 3.562408164725639e-05\n",
      "Epoch 332, Loss: 0.006352344527840614\n",
      "Epoch 333, Loss: 0.0001330220839008689\n",
      "Epoch 334, Loss: 0.00045180917368270457\n",
      "Epoch 335, Loss: 0.0012556249275803566\n",
      "Epoch 336, Loss: 0.0005343485390767455\n",
      "Epoch 337, Loss: 0.001546346116811037\n",
      "Epoch 338, Loss: 0.03600948303937912\n",
      "Epoch 339, Loss: 0.013487383723258972\n",
      "Epoch 340, Loss: 0.0002738939947448671\n",
      "Epoch 341, Loss: 0.011042025871574879\n",
      "Epoch 342, Loss: 0.00325941713526845\n",
      "Epoch 343, Loss: 0.053802888840436935\n",
      "Epoch 344, Loss: 0.0035455210600048304\n",
      "Epoch 345, Loss: 0.0019060588674619794\n",
      "Epoch 346, Loss: 0.0011191777884960175\n",
      "Epoch 347, Loss: 0.0003101241309195757\n",
      "Epoch 348, Loss: 0.0008246747893281281\n",
      "Epoch 349, Loss: 0.003907990641891956\n",
      "Epoch 350, Loss: 0.0008038089144974947\n",
      "Epoch 351, Loss: 0.0008861527312546968\n",
      "Epoch 352, Loss: 0.00040352364885620773\n",
      "Epoch 353, Loss: 0.004873938392847776\n",
      "Epoch 354, Loss: 0.003956064116209745\n",
      "Epoch 355, Loss: 0.001891362713649869\n",
      "Epoch 356, Loss: 0.0003255474439356476\n",
      "Epoch 357, Loss: 0.0008383532986044884\n",
      "Epoch 358, Loss: 0.0026296419091522694\n",
      "Epoch 359, Loss: 0.000985564198344946\n",
      "Epoch 360, Loss: 0.0014628871576860547\n",
      "Epoch 361, Loss: 0.00022723848815076053\n",
      "Epoch 362, Loss: 8.036999497562647e-05\n",
      "Epoch 363, Loss: 0.00030994953704066575\n",
      "Epoch 364, Loss: 0.0007857014425098896\n",
      "Epoch 365, Loss: 0.0016830303939059377\n",
      "Epoch 366, Loss: 0.0003863721794914454\n",
      "Epoch 367, Loss: 0.0002708399260882288\n",
      "Epoch 368, Loss: 0.0009494338883087039\n",
      "Epoch 369, Loss: 0.00022098103363532573\n",
      "Epoch 370, Loss: 0.0010551756713539362\n",
      "Epoch 371, Loss: 0.0005092526553198695\n",
      "Epoch 372, Loss: 0.0015223448863252997\n",
      "Epoch 373, Loss: 0.001059341593645513\n",
      "Epoch 374, Loss: 0.0026258775033056736\n",
      "Epoch 375, Loss: 0.0012154086725786328\n",
      "Epoch 376, Loss: 0.0002473181812092662\n",
      "Epoch 377, Loss: 0.00010356873099226505\n",
      "Epoch 378, Loss: 0.00019222924311179668\n",
      "Epoch 379, Loss: 0.006672145798802376\n",
      "Epoch 380, Loss: 0.0014527302701026201\n",
      "Epoch 381, Loss: 0.00013339986617211252\n",
      "Epoch 382, Loss: 0.007613341324031353\n",
      "Epoch 383, Loss: 0.0007456891471520066\n",
      "Epoch 384, Loss: 0.0030045725870877504\n",
      "Epoch 385, Loss: 0.002603130880743265\n",
      "Epoch 386, Loss: 0.00021883597946725786\n",
      "Epoch 387, Loss: 0.0006939193117432296\n",
      "Epoch 388, Loss: 0.00011711310071405023\n",
      "Epoch 389, Loss: 0.004760140553116798\n",
      "Epoch 390, Loss: 0.003424455877393484\n",
      "Epoch 391, Loss: 0.0022570618893951178\n",
      "Epoch 392, Loss: 0.0024166235234588385\n",
      "Epoch 393, Loss: 0.0016193167539313436\n",
      "Epoch 394, Loss: 0.0004571385507006198\n",
      "Epoch 395, Loss: 0.002574633341282606\n",
      "Epoch 396, Loss: 0.0006933359545655549\n",
      "Epoch 397, Loss: 0.000643262465018779\n",
      "Epoch 398, Loss: 0.00036650881520472467\n",
      "Epoch 399, Loss: 0.0013805270427837968\n",
      "Epoch 400, Loss: 0.002454254077747464\n",
      "Epoch 401, Loss: 0.000244867114815861\n",
      "Epoch 402, Loss: 0.00022182044631335884\n",
      "Epoch 403, Loss: 0.0014997088583186269\n",
      "Epoch 404, Loss: 0.0001600679534021765\n",
      "Epoch 405, Loss: 0.0007663469295948744\n",
      "Epoch 406, Loss: 0.001308824634179473\n",
      "Epoch 407, Loss: 0.003986126743257046\n",
      "Epoch 408, Loss: 0.0013962473021820188\n",
      "Epoch 409, Loss: 0.0012468822533264756\n",
      "Epoch 410, Loss: 0.004874491132795811\n",
      "Epoch 411, Loss: 0.004092345014214516\n",
      "Epoch 412, Loss: 0.004545459058135748\n",
      "Epoch 413, Loss: 0.0007952287560328841\n",
      "Epoch 414, Loss: 0.001617085887119174\n",
      "Epoch 415, Loss: 0.01719183847308159\n",
      "Epoch 416, Loss: 0.0011043427512049675\n",
      "Epoch 417, Loss: 0.006347612477838993\n",
      "Epoch 418, Loss: 0.002664289204403758\n",
      "Epoch 419, Loss: 0.0019163851393386722\n",
      "Epoch 420, Loss: 0.0038069221191108227\n",
      "Epoch 421, Loss: 0.0009793011704459786\n",
      "Epoch 422, Loss: 0.0022403206676244736\n",
      "Epoch 423, Loss: 0.0009306252468377352\n",
      "Epoch 424, Loss: 0.004080756567418575\n",
      "Epoch 425, Loss: 0.0015199510380625725\n",
      "Epoch 426, Loss: 0.002370560308918357\n",
      "Epoch 427, Loss: 0.0077103315852582455\n",
      "Epoch 428, Loss: 0.0004808566009160131\n",
      "Epoch 429, Loss: 0.02159631997346878\n",
      "Epoch 430, Loss: 0.002557281870394945\n",
      "Epoch 431, Loss: 0.005538234952837229\n",
      "Epoch 432, Loss: 0.0015513361431658268\n",
      "Epoch 433, Loss: 0.0004926242399960756\n",
      "Epoch 434, Loss: 0.0018959467997774482\n",
      "Epoch 435, Loss: 0.0025394714903086424\n",
      "Epoch 436, Loss: 0.0022496203891932964\n",
      "Epoch 437, Loss: 9.516037243884057e-05\n",
      "Epoch 438, Loss: 0.0013028036337345839\n",
      "Epoch 439, Loss: 6.959363963687792e-05\n",
      "Epoch 440, Loss: 0.0010486209066584706\n",
      "Epoch 441, Loss: 0.00041766275535337627\n",
      "Epoch 442, Loss: 0.0009646288235671818\n",
      "Epoch 443, Loss: 0.00129180948715657\n",
      "Epoch 444, Loss: 0.0005374880274757743\n",
      "Epoch 445, Loss: 0.0011716028675436974\n",
      "Epoch 446, Loss: 0.001055453554727137\n",
      "Epoch 447, Loss: 0.0006385453743860126\n",
      "Epoch 448, Loss: 0.00048602785682305694\n",
      "Epoch 449, Loss: 0.00022363693278748542\n",
      "Epoch 450, Loss: 0.005280558485537767\n",
      "Epoch 451, Loss: 0.00020518178644124418\n",
      "Epoch 452, Loss: 0.023044822737574577\n",
      "Epoch 453, Loss: 0.03518686071038246\n",
      "Epoch 454, Loss: 0.013069907203316689\n",
      "Epoch 455, Loss: 0.0006123388302512467\n",
      "Epoch 456, Loss: 0.0007156161591410637\n",
      "Epoch 457, Loss: 0.0013614173512905836\n",
      "Epoch 458, Loss: 0.01149025373160839\n",
      "Epoch 459, Loss: 0.0037569254636764526\n",
      "Epoch 460, Loss: 0.0031854789704084396\n",
      "Epoch 461, Loss: 0.006319262553006411\n",
      "Epoch 462, Loss: 0.00025284188450314105\n",
      "Epoch 463, Loss: 0.00029638520209118724\n",
      "Epoch 464, Loss: 0.0011670091189444065\n",
      "Epoch 465, Loss: 9.914373367791995e-05\n",
      "Epoch 466, Loss: 0.00035066803684458137\n",
      "Epoch 467, Loss: 0.0005646848003380001\n",
      "Epoch 468, Loss: 0.007085525430738926\n",
      "Epoch 469, Loss: 0.0009223948582075536\n",
      "Epoch 470, Loss: 7.750950317131355e-05\n",
      "Epoch 471, Loss: 0.0004704937746282667\n",
      "Epoch 472, Loss: 0.004802265204489231\n",
      "Epoch 473, Loss: 0.00027160823810845613\n",
      "Epoch 474, Loss: 0.003225306747481227\n",
      "Epoch 475, Loss: 0.0019250436453148723\n",
      "Epoch 476, Loss: 0.0031890778336673975\n",
      "Epoch 477, Loss: 0.002355901524424553\n",
      "Epoch 478, Loss: 0.0036097229458391666\n",
      "Epoch 479, Loss: 0.00015123227785807103\n",
      "Epoch 480, Loss: 0.000859393912833184\n",
      "Epoch 481, Loss: 0.0005375769687816501\n",
      "Epoch 482, Loss: 0.00172764144372195\n",
      "Epoch 483, Loss: 0.006567545235157013\n",
      "Epoch 484, Loss: 0.007819805294275284\n",
      "Epoch 485, Loss: 0.01650051213800907\n",
      "Epoch 486, Loss: 0.00045074158697389066\n",
      "Epoch 487, Loss: 0.0039451546035707\n",
      "Epoch 488, Loss: 0.0004613726050592959\n",
      "Epoch 489, Loss: 0.0005082296556793153\n",
      "Epoch 490, Loss: 0.004480281844735146\n",
      "Epoch 491, Loss: 0.016008099541068077\n",
      "Epoch 492, Loss: 0.0001250133354915306\n",
      "Epoch 493, Loss: 0.0026488658040761948\n",
      "Epoch 494, Loss: 0.004711905959993601\n",
      "Epoch 495, Loss: 0.009011228568851948\n",
      "Epoch 496, Loss: 0.010721663013100624\n",
      "Epoch 497, Loss: 0.003582358593121171\n",
      "Epoch 498, Loss: 0.0006810730556026101\n",
      "Epoch 499, Loss: 0.0034388205967843533\n",
      "Epoch 500, Loss: 0.005139936693012714\n",
      "Epoch 501, Loss: 0.000821079476736486\n",
      "Epoch 502, Loss: 0.0012039855355396867\n",
      "Epoch 503, Loss: 0.00019754561071749777\n",
      "Epoch 504, Loss: 0.0006287968717515469\n",
      "Epoch 505, Loss: 0.0015784306451678276\n",
      "Epoch 506, Loss: 0.04784810170531273\n",
      "Epoch 507, Loss: 0.0005199445877224207\n",
      "Epoch 508, Loss: 0.000896158569958061\n",
      "Epoch 509, Loss: 0.0006527341902256012\n",
      "Epoch 510, Loss: 0.0005471957847476006\n",
      "Epoch 511, Loss: 0.00013818735897075385\n",
      "Epoch 512, Loss: 0.012408657930791378\n",
      "Epoch 513, Loss: 0.006625525187700987\n",
      "Epoch 514, Loss: 0.0003500221064314246\n",
      "Epoch 515, Loss: 0.0005605381447821856\n",
      "Epoch 516, Loss: 0.0013968304265290499\n",
      "Epoch 517, Loss: 0.00022641992836724967\n",
      "Epoch 518, Loss: 7.607173029100522e-05\n",
      "Epoch 519, Loss: 0.0027489042840898037\n",
      "Epoch 520, Loss: 0.0025488603860139847\n",
      "Epoch 521, Loss: 0.005444252863526344\n",
      "Epoch 522, Loss: 0.06092650070786476\n",
      "Epoch 523, Loss: 0.033507607877254486\n",
      "Epoch 524, Loss: 0.014117911458015442\n",
      "Epoch 525, Loss: 0.0003333347849547863\n",
      "Epoch 526, Loss: 0.02214888669550419\n",
      "Epoch 527, Loss: 0.0009005583124235272\n",
      "Epoch 528, Loss: 0.00136880564969033\n",
      "Epoch 529, Loss: 0.00019892517593689263\n",
      "Epoch 530, Loss: 0.0007128005381673574\n",
      "Epoch 531, Loss: 0.00020696071442216635\n",
      "Epoch 532, Loss: 0.0003232235903851688\n",
      "Epoch 533, Loss: 0.00031982152722775936\n",
      "Epoch 534, Loss: 0.006818889174610376\n",
      "Epoch 535, Loss: 0.012976991944015026\n",
      "Epoch 536, Loss: 0.02106098271906376\n",
      "Epoch 537, Loss: 0.0005566641921177506\n",
      "Epoch 538, Loss: 0.00013424399367067963\n",
      "Epoch 539, Loss: 0.0016347883502021432\n",
      "Epoch 540, Loss: 0.0004519364156294614\n",
      "Epoch 541, Loss: 0.00021659016783814877\n",
      "Epoch 542, Loss: 0.00019633329065982252\n",
      "Epoch 543, Loss: 0.002898749429732561\n",
      "Epoch 544, Loss: 5.690987381967716e-05\n",
      "Epoch 545, Loss: 0.0003909301303792745\n",
      "Epoch 546, Loss: 0.005870310124009848\n",
      "Epoch 547, Loss: 0.00023033720208331943\n",
      "Epoch 548, Loss: 0.00012178572069387883\n",
      "Epoch 549, Loss: 0.00022852087568026036\n",
      "Epoch 550, Loss: 2.7649797630147077e-05\n",
      "Epoch 551, Loss: 0.00025776700931601226\n",
      "Epoch 552, Loss: 3.266392013756558e-05\n",
      "Epoch 553, Loss: 0.0019472269341349602\n",
      "Epoch 554, Loss: 0.0005635670968331397\n",
      "Epoch 555, Loss: 0.000153045475599356\n",
      "Epoch 556, Loss: 7.543055107817054e-05\n",
      "Epoch 557, Loss: 0.0002059394319076091\n",
      "Epoch 558, Loss: 0.000529030745383352\n",
      "Epoch 559, Loss: 0.003541855840012431\n",
      "Epoch 560, Loss: 0.0003690668672788888\n",
      "Epoch 561, Loss: 0.04166287183761597\n",
      "Epoch 562, Loss: 0.0007481520297005773\n",
      "Epoch 563, Loss: 0.002987444633617997\n",
      "Epoch 564, Loss: 0.001075079315342009\n",
      "Epoch 565, Loss: 0.0018675911705940962\n",
      "Epoch 566, Loss: 0.0005479562096297741\n",
      "Epoch 567, Loss: 0.0007765101036056876\n",
      "Epoch 568, Loss: 0.0005032101762481034\n",
      "Epoch 569, Loss: 0.0004110836598556489\n",
      "Epoch 570, Loss: 0.0006730793975293636\n",
      "Epoch 571, Loss: 0.004817376844584942\n",
      "Epoch 572, Loss: 0.001182860811240971\n",
      "Epoch 573, Loss: 0.004391361027956009\n",
      "Epoch 574, Loss: 0.0020038040820509195\n",
      "Epoch 575, Loss: 0.026668118312954903\n",
      "Epoch 576, Loss: 0.0030453582294285297\n",
      "Epoch 577, Loss: 0.07472193241119385\n",
      "Epoch 578, Loss: 0.013113073073327541\n",
      "Epoch 579, Loss: 0.05076443403959274\n",
      "Epoch 580, Loss: 0.0027862500865012407\n",
      "Epoch 581, Loss: 7.575751806143671e-05\n",
      "Epoch 582, Loss: 0.0009388271137140691\n",
      "Epoch 583, Loss: 0.0007457399624399841\n",
      "Epoch 584, Loss: 0.0019074674928560853\n",
      "Epoch 585, Loss: 0.0011898238444700837\n",
      "Epoch 586, Loss: 0.0017689184751361609\n",
      "Epoch 587, Loss: 0.00011011082824552432\n",
      "Epoch 588, Loss: 0.0005044037825427949\n",
      "Epoch 589, Loss: 0.0005163544556125998\n",
      "Epoch 590, Loss: 0.00016553964815102518\n",
      "Epoch 591, Loss: 8.156023977790028e-05\n",
      "Epoch 592, Loss: 0.0019853778649121523\n",
      "Epoch 593, Loss: 0.00073312281165272\n",
      "Epoch 594, Loss: 0.00031369703356176615\n",
      "Epoch 595, Loss: 0.0013348496286198497\n",
      "Epoch 596, Loss: 0.000429586973041296\n",
      "Epoch 597, Loss: 0.0014447197318077087\n",
      "Epoch 598, Loss: 0.0002205648343078792\n",
      "Epoch 599, Loss: 0.003007499035447836\n",
      "Epoch 600, Loss: 0.0002389263827353716\n"
     ]
    }
   ],
   "source": [
    "model = MultiLabelNN()\n",
    "\n",
    "criterion = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 루프\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=600):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ True,  True, False, False,  True,  True,  True],\n",
      "       [False, False, False, False,  True,  True,  True],\n",
      "       [ True,  True, False, False,  True,  True,  True],\n",
      "       [ True, False, False, False, False, False,  True],\n",
      "       [False, False, False, False,  True, False,  True],\n",
      "       [ True,  True, False, False, False, False, False],\n",
      "       [False, False,  True,  True,  True, False,  True],\n",
      "       [False,  True,  True, False, False, False,  True],\n",
      "       [ True,  True,  True,  True, False, False, False],\n",
      "       [False,  True, False, False,  True,  True,  True],\n",
      "       [False, False, False,  True,  True,  True,  True],\n",
      "       [False,  True,  True,  True,  True, False,  True],\n",
      "       [False, False, False, False,  True,  True,  True],\n",
      "       [False, False, False,  True,  True,  True,  True],\n",
      "       [False,  True,  True,  True,  True,  True,  True],\n",
      "       [False,  True, False, False,  True,  True,  True],\n",
      "       [False,  True,  True, False, False, False, False],\n",
      "       [False, False, False,  True,  True, False, False],\n",
      "       [False, False, False, False,  True,  True,  True],\n",
      "       [False, False, False, False,  True,  True,  True],\n",
      "       [False,  True,  True, False, False, False, False],\n",
      "       [ True, False, False,  True,  True, False,  True],\n",
      "       [ True,  True, False, False, False, False, False],\n",
      "       [ True,  True, False, False, False, False,  True],\n",
      "       [ True, False, False,  True,  True, False,  True],\n",
      "       [False, False, False, False,  True,  True,  True],\n",
      "       [ True,  True, False,  True,  True,  True,  True],\n",
      "       [False, False, False,  True,  True,  True,  True],\n",
      "       [ True,  True, False,  True,  True, False,  True],\n",
      "       [ True, False, False,  True,  True, False,  True]])]\n",
      "Average Loss: 11.9440, Precision: 0.5131, Recall: 0.5076, F1 Score: 0.5046\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    criterion = nn.BCELoss()  # 이진 크로스 엔트로피 손실\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            # 예측값을 0 또는 1로 변환\n",
    "            predicted = outputs > 0.5\n",
    "            all_predictions.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "    print(all_predictions)\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # 스코어 계산\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro')\n",
    "    \n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Average Loss: {average_loss:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "evaluate_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
